{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11dd42e-79c9-4423-bd15-d0c8ca3508d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    with smart_open.open(url, \"rb\") as file:\n",
    "        with tarfile.open(fileobj=file) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
    "                    member_bytes = tar.extractfile(member).read()\n",
    "                    yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402ae8d1-33e9-4f6c-a800-5f282bc8d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "387 \n",
      "Neural Net and Traditional Classifiers  \n",
      "William Y. Huang and Richard P. Lippmann \n",
      "MIT Lincoln Laboratory \n",
      "Lexington, MA 02173, USA \n",
      "Abstract\n",
      "Previous work on nets with continuous-valued inputs led to generative \n",
      "procedures to construct convex decision regions with two-layer percepttons (one hidden \n",
      "layer) and arbitrary decision regions with three-layer percepttons (two hidden layers). \n",
      "Here we demonstrate that two-layer perceptton classifiers trained with back propagation \n",
      "can form both c\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "490d4ac3-9db3-4fbb-b79c-ecf83df1d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a20d90-2ee2-428f-bbca-6dc4b36dd8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052565b-af1d-4030-bafb-e448fd43a43d",
   "metadata": {},
   "source": [
    "     Interpreting the result using \"no_above = .75 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c1d26e-8114-4fad-8c17-f804eab03078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462085f2-f78d-4844-9626-3c88ecddf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8a8b92-9978-41f6-ae84-2da1879878dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 6754\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c058a33-06e2-4fac-9cef-96fca4656467",
   "metadata": {},
   "source": [
    "     Interpreting the result using \"no_above = .9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b565b8-db2a-457d-ae6e-d69245e49ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 6798\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.9)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913fead-49a5-4191-89b3-7d7ab7ccb965",
   "metadata": {},
   "source": [
    "The no_above parameter specifies the maximum proportion of documents in which a token can appear to be included in the dictionary. By setting it to 0.75, tokens that appear in more than 75% of the documents are excluded from the dictionary. Similarly, setting it to 0.9 excludes tokens that appear in more than 90% of the documents.\n",
    "\n",
    "The difference in the number of unique tokens between the two settings reflects the stricter filtering applied when no_above is set to 0.9, resulting in fewer tokens being included in the dictionary. However, the number of documents remains the same in both cases since it is independent of the dictionary filtering.\n",
    "\n",
    "These changes affect the vocabulary size and potentially the quality of the bag-of-words representation. Adjusting no_above allows for control over the vocabulary size and the inclusion of more or fewer frequent terms in the dictionary based on the specific requirements of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfed6f7-b40f-49fa-b5fb-cefdcfc8401c",
   "metadata": {},
   "source": [
    "    Interpreting the results after training the LDA model using num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "062fee64-5ea4-4e69-92ab-0dd1c1b8c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -0.6771.\n",
      "[([(0.031453587, 'network'),\n",
      "   (0.019327786, 'learning'),\n",
      "   (0.015130068, 'unit'),\n",
      "   (0.011830874, 'input'),\n",
      "   (0.011249237, 'weight'),\n",
      "   (0.009832398, 'training'),\n",
      "   (0.009467361, 'output'),\n",
      "   (0.008128904, 'error'),\n",
      "   (0.0080774445, 'time'),\n",
      "   (0.0078066825, 'hidden'),\n",
      "   (0.006048526, 'state'),\n",
      "   (0.0059344866, 'layer'),\n",
      "   (0.005556106, 'problem'),\n",
      "   (0.005482669, 'figure'),\n",
      "   (0.0053764316, 'task'),\n",
      "   (0.0050665326, 'net'),\n",
      "   (0.0049162614, 'control'),\n",
      "   (0.0047234627, 'algorithm'),\n",
      "   (0.0046429625, 'value'),\n",
      "   (0.0040557957, 'number')],\n",
      "  -0.37357946210282633),\n",
      " ([(0.010582791, 'learning'),\n",
      "   (0.010535968, 'algorithm'),\n",
      "   (0.0077663343, 'state'),\n",
      "   (0.0072821225, 'value'),\n",
      "   (0.006413359, 'error'),\n",
      "   (0.0060458826, 'problem'),\n",
      "   (0.005713942, 'example'),\n",
      "   (0.0052246987, 'then'),\n",
      "   (0.0052204872, 'probability'),\n",
      "   (0.005070462, 'number'),\n",
      "   (0.0049493858, 'case'),\n",
      "   (0.0048729638, 'model'),\n",
      "   (0.0048715696, 'bound'),\n",
      "   (0.004487168, 'optimal'),\n",
      "   (0.0043733516, 'method'),\n",
      "   (0.004306704, 'given'),\n",
      "   (0.0041845194, 'will'),\n",
      "   (0.0040678456, 'let'),\n",
      "   (0.0040607527, 'training'),\n",
      "   (0.00400842, 'distribution')],\n",
      "  -0.4190569735347176),\n",
      " ([(0.0167499, 'network'),\n",
      "   (0.014856224, 'training'),\n",
      "   (0.012725713, 'recognition'),\n",
      "   (0.010737555, 'speech'),\n",
      "   (0.009174753, 'classifier'),\n",
      "   (0.008953723, 'model'),\n",
      "   (0.008389637, 'word'),\n",
      "   (0.0072420514, 'data'),\n",
      "   (0.0072033256, 'output'),\n",
      "   (0.006874945, 'input'),\n",
      "   (0.006623992, 'classification'),\n",
      "   (0.00657498, 'performance'),\n",
      "   (0.0065159933, 'error'),\n",
      "   (0.006472891, 'time'),\n",
      "   (0.0063633216, 'were'),\n",
      "   (0.006105883, 'class'),\n",
      "   (0.0059007667, 'feature'),\n",
      "   (0.0056281746, 'test'),\n",
      "   (0.005106672, 'state'),\n",
      "   (0.0049081305, 'probability')],\n",
      "  -0.4962464157771583),\n",
      " ([(0.016016055, 'model'),\n",
      "   (0.015358552, 'data'),\n",
      "   (0.00954956, 'algorithm'),\n",
      "   (0.0066339434, 'distribution'),\n",
      "   (0.0065661473, 'parameter'),\n",
      "   (0.006214807, 'network'),\n",
      "   (0.006110063, 'vector'),\n",
      "   (0.005927878, 'matrix'),\n",
      "   (0.005856785, 'method'),\n",
      "   (0.005571619, 'point'),\n",
      "   (0.005269188, 'gaussian'),\n",
      "   (0.0052043037, 'learning'),\n",
      "   (0.005149308, 'problem'),\n",
      "   (0.0050496245, 'space'),\n",
      "   (0.0048978976, 'variable'),\n",
      "   (0.0045900126, 'mixture'),\n",
      "   (0.004460463, 'density'),\n",
      "   (0.004385477, 'mean'),\n",
      "   (0.0043722843, 'given'),\n",
      "   (0.0042932834, 'component')],\n",
      "  -0.5410629674051222),\n",
      " ([(0.010112912, 'rule'),\n",
      "   (0.009792329, 'representation'),\n",
      "   (0.008326676, 'unit'),\n",
      "   (0.007290785, 'structure'),\n",
      "   (0.0071379477, 'memory'),\n",
      "   (0.0069483784, 'pattern'),\n",
      "   (0.005958087, 'feature'),\n",
      "   (0.005902892, 'code'),\n",
      "   (0.0057044313, 'input'),\n",
      "   (0.0054832874, 'sequence'),\n",
      "   (0.005340511, 'vector'),\n",
      "   (0.00523988, 'network'),\n",
      "   (0.0051014507, 'model'),\n",
      "   (0.005065704, 'node'),\n",
      "   (0.005021191, 'example'),\n",
      "   (0.0050138654, 'level'),\n",
      "   (0.0048799156, 'language'),\n",
      "   (0.0045217928, 'tree'),\n",
      "   (0.0042463867, 'number'),\n",
      "   (0.003882965, 'information')],\n",
      "  -0.72376823669247),\n",
      " ([(0.023986997, 'neuron'),\n",
      "   (0.017058479, 'network'),\n",
      "   (0.0131792575, 'model'),\n",
      "   (0.012412755, 'time'),\n",
      "   (0.007474778, 'state'),\n",
      "   (0.0074529974, 'input'),\n",
      "   (0.006812677, 'spike'),\n",
      "   (0.00621661, 'dynamic'),\n",
      "   (0.006123769, 'pattern'),\n",
      "   (0.0059716804, 'synaptic'),\n",
      "   (0.005451424, 'memory'),\n",
      "   (0.0051744035, 'firing'),\n",
      "   (0.0045797685, 'signal'),\n",
      "   (0.004541472, 'figure'),\n",
      "   (0.0044494797, 'activity'),\n",
      "   (0.004219447, 'noise'),\n",
      "   (0.004143767, 'delay'),\n",
      "   (0.0039133383, 'information'),\n",
      "   (0.0038542917, 'potential'),\n",
      "   (0.0036977034, 'when')],\n",
      "  -0.7740711849851933),\n",
      " ([(0.02213917, 'cell'),\n",
      "   (0.012922015, 'model'),\n",
      "   (0.010072899, 'input'),\n",
      "   (0.009853356, 'response'),\n",
      "   (0.009647913, 'stimulus'),\n",
      "   (0.008699479, 'neuron'),\n",
      "   (0.0067134593, 'figure'),\n",
      "   (0.0062215147, 'visual'),\n",
      "   (0.006097205, 'activity'),\n",
      "   (0.0056385477, 'cortex'),\n",
      "   (0.0049551423, 'field'),\n",
      "   (0.0047429698, 'orientation'),\n",
      "   (0.004555751, 'cortical'),\n",
      "   (0.004426597, 'pattern'),\n",
      "   (0.004319174, 'unit'),\n",
      "   (0.004221091, 'time'),\n",
      "   (0.0038589933, 'receptive'),\n",
      "   (0.0037858756, 'connection'),\n",
      "   (0.0037751212, 'map'),\n",
      "   (0.0036181333, 'layer')],\n",
      "  -0.7815706943276531),\n",
      " ([(0.02436522, 'image'),\n",
      "   (0.010977649, 'object'),\n",
      "   (0.0092319995, 'model'),\n",
      "   (0.008083203, 'figure'),\n",
      "   (0.007269811, 'motion'),\n",
      "   (0.0071536875, 'visual'),\n",
      "   (0.006174489, 'feature'),\n",
      "   (0.005273029, 'filter'),\n",
      "   (0.0048627337, 'pixel'),\n",
      "   (0.004818142, 'field'),\n",
      "   (0.004742765, 'position'),\n",
      "   (0.004721557, 'face'),\n",
      "   (0.0043833903, 'direction'),\n",
      "   (0.004209523, 'were'),\n",
      "   (0.0041268542, 'information'),\n",
      "   (0.0040181773, 'representation'),\n",
      "   (0.0038075, 'location'),\n",
      "   (0.0037928629, 'based'),\n",
      "   (0.0037680762, 'recognition'),\n",
      "   (0.0036884565, 'view')],\n",
      "  -0.7901835169125251),\n",
      " ([(0.008218404, 'correlation'),\n",
      "   (0.008055021, 'field'),\n",
      "   (0.0077194585, 'network'),\n",
      "   (0.007267307, 'weight'),\n",
      "   (0.0070970496, 'learning'),\n",
      "   (0.0063459775, 'order'),\n",
      "   (0.006162417, 'equation'),\n",
      "   (0.0054433662, 'solution'),\n",
      "   (0.0053394283, 'unit'),\n",
      "   (0.0052412073, 'value'),\n",
      "   (0.0050765495, 'phase'),\n",
      "   (0.005021849, 'energy'),\n",
      "   (0.0050018313, 'generalization'),\n",
      "   (0.0049899775, 'student'),\n",
      "   (0.0049856557, 'case'),\n",
      "   (0.0048226407, 'teacher'),\n",
      "   (0.004813141, 'model'),\n",
      "   (0.004712429, 'training'),\n",
      "   (0.0045567257, 'line'),\n",
      "   (0.004547605, 'mean')],\n",
      "  -0.8677625408943255),\n",
      " ([(0.018518163, 'network'),\n",
      "   (0.013219201, 'circuit'),\n",
      "   (0.012135119, 'input'),\n",
      "   (0.0105182845, 'chip'),\n",
      "   (0.010141925, 'output'),\n",
      "   (0.009601041, 'analog'),\n",
      "   (0.008817554, 'weight'),\n",
      "   (0.007878872, 'figure'),\n",
      "   (0.006942134, 'neuron'),\n",
      "   (0.0067123985, 'time'),\n",
      "   (0.0064035184, 'current'),\n",
      "   (0.0060551525, 'voltage'),\n",
      "   (0.0051602884, 'threshold'),\n",
      "   (0.0048299474, 'vlsi'),\n",
      "   (0.0046892627, 'bit'),\n",
      "   (0.004617915, 'implementation'),\n",
      "   (0.0045377845, 'gate'),\n",
      "   (0.0042027663, 'number'),\n",
      "   (0.003836793, 'processor'),\n",
      "   (0.003830554, 'signal')],\n",
      "  -1.0036830649279185)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = 50  \n",
    "\n",
    "\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "\n",
    "top_topics = model.top_topics(corpus)\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n",
    "\n",
    "def check_topic_threshold(x, topic,threshold):\n",
    "    topics = model.get_document_topics(corpus[x])\n",
    "    for i in topics:\n",
    "        if i[0]==topic and i[1]>threshold:\n",
    "            return True\n",
    "    else: return False\n",
    "t=[docs[x] for x in range(len(corpus)) if check_topic_threshold(x, 0, .9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5b6f4-af8a-4d24-b40c-c0a103b3de9c",
   "metadata": {},
   "source": [
    "    Interpreting the results after training the LDA model using num_topics = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17d85e49-22d0-4d04-8b86-0539ac7f1931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -0.7639.\n",
      "[([(0.011681974, 'network'),\n",
      "   (0.008170731, 'bound'),\n",
      "   (0.007343298, 'then'),\n",
      "   (0.0072680283, 'algorithm'),\n",
      "   (0.0071850433, 'number'),\n",
      "   (0.0069085457, 'let'),\n",
      "   (0.006411364, 'theorem'),\n",
      "   (0.0063346955, 'case'),\n",
      "   (0.006192115, 'probability'),\n",
      "   (0.006028156, 'any'),\n",
      "   (0.0055739232, 'learning'),\n",
      "   (0.0055321176, 'given'),\n",
      "   (0.0053626937, 'class'),\n",
      "   (0.0052585984, 'example'),\n",
      "   (0.005192798, 'will'),\n",
      "   (0.00470866, 'distribution'),\n",
      "   (0.004705537, 'some'),\n",
      "   (0.00464571, 'point'),\n",
      "   (0.004587281, 'vector'),\n",
      "   (0.0045482274, 'there')],\n",
      "  -0.33981173187597286),\n",
      " ([(0.034033764, 'network'),\n",
      "   (0.021554386, 'unit'),\n",
      "   (0.014828183, 'input'),\n",
      "   (0.010212382, 'output'),\n",
      "   (0.010178825, 'training'),\n",
      "   (0.009407514, 'hidden'),\n",
      "   (0.008490284, 'layer'),\n",
      "   (0.007414752, 'pattern'),\n",
      "   (0.0070998813, 'net'),\n",
      "   (0.0068112053, 'weight'),\n",
      "   (0.00654177, 'recognition'),\n",
      "   (0.006393011, 'learning'),\n",
      "   (0.0056892936, 'word'),\n",
      "   (0.0051678964, 'time'),\n",
      "   (0.005120702, 'were'),\n",
      "   (0.0050691846, 'task'),\n",
      "   (0.0050276737, 'architecture'),\n",
      "   (0.004508442, 'figure'),\n",
      "   (0.004370956, 'number'),\n",
      "   (0.0042392937, 'representation')],\n",
      "  -0.4444563070104542),\n",
      " ([(0.01639351, 'network'),\n",
      "   (0.016363386, 'model'),\n",
      "   (0.016218808, 'training'),\n",
      "   (0.014603046, 'data'),\n",
      "   (0.010079855, 'classifier'),\n",
      "   (0.008271143, 'error'),\n",
      "   (0.007601057, 'performance'),\n",
      "   (0.007315561, 'speech'),\n",
      "   (0.0062279445, 'test'),\n",
      "   (0.006227519, 'class'),\n",
      "   (0.0061131595, 'input'),\n",
      "   (0.0057006623, 'classification'),\n",
      "   (0.0056192386, 'were'),\n",
      "   (0.0055441395, 'probability'),\n",
      "   (0.0054572592, 'feature'),\n",
      "   (0.00515749, 'number'),\n",
      "   (0.0050789644, 'recognition'),\n",
      "   (0.0050093653, 'output'),\n",
      "   (0.004940411, 'method'),\n",
      "   (0.004672015, 'time')],\n",
      "  -0.4569387097525678),\n",
      " ([(0.014513696, 'error'),\n",
      "   (0.011425991, 'training'),\n",
      "   (0.010501014, 'data'),\n",
      "   (0.008428316, 'learning'),\n",
      "   (0.007773247, 'example'),\n",
      "   (0.0077657457, 'problem'),\n",
      "   (0.0074531464, 'algorithm'),\n",
      "   (0.0072384113, 'kernel'),\n",
      "   (0.006373825, 'regression'),\n",
      "   (0.0057835784, 'linear'),\n",
      "   (0.0055931597, 'case'),\n",
      "   (0.005505051, 'generalization'),\n",
      "   (0.0054550883, 'value'),\n",
      "   (0.0052407384, 'optimal'),\n",
      "   (0.0051638028, 'method'),\n",
      "   (0.005026651, 'vector'),\n",
      "   (0.004744569, 'point'),\n",
      "   (0.004657871, 'space'),\n",
      "   (0.00448675, 'estimate'),\n",
      "   (0.004388716, 'will')],\n",
      "  -0.4839708580351176),\n",
      " ([(0.01635244, 'model'),\n",
      "   (0.011541082, 'data'),\n",
      "   (0.009779747, 'algorithm'),\n",
      "   (0.009298203, 'distribution'),\n",
      "   (0.008387922, 'parameter'),\n",
      "   (0.0070614154, 'gaussian'),\n",
      "   (0.0065901265, 'method'),\n",
      "   (0.0065741464, 'network'),\n",
      "   (0.0057571353, 'mixture'),\n",
      "   (0.0056285867, 'variable'),\n",
      "   (0.0055373763, 'probability'),\n",
      "   (0.0054945177, 'mean'),\n",
      "   (0.0052213627, 'vector'),\n",
      "   (0.005117786, 'learning'),\n",
      "   (0.005104394, 'density'),\n",
      "   (0.0049274573, 'given'),\n",
      "   (0.0046732393, 'likelihood'),\n",
      "   (0.004603387, 'problem'),\n",
      "   (0.0044744182, 'matrix'),\n",
      "   (0.0044258493, 'point')],\n",
      "  -0.5309167331261746),\n",
      " ([(0.020919058, 'signal'),\n",
      "   (0.01581026, 'time'),\n",
      "   (0.012097844, 'frequency'),\n",
      "   (0.011175066, 'noise'),\n",
      "   (0.008723843, 'spike'),\n",
      "   (0.008365468, 'information'),\n",
      "   (0.007972549, 'filter'),\n",
      "   (0.00784701, 'response'),\n",
      "   (0.006576975, 'stimulus'),\n",
      "   (0.006422999, 'channel'),\n",
      "   (0.0056259907, 'figure'),\n",
      "   (0.005619834, 'rate'),\n",
      "   (0.005425535, 'temporal'),\n",
      "   (0.004794171, 'were'),\n",
      "   (0.0047209975, 'model'),\n",
      "   (0.004667984, 'auditory'),\n",
      "   (0.004478037, 'input'),\n",
      "   (0.0041715326, 'processing'),\n",
      "   (0.0041692206, 'output'),\n",
      "   (0.004020215, 'show')],\n",
      "  -0.6244007537508813),\n",
      " ([(0.023206051, 'learning'),\n",
      "   (0.013634347, 'algorithm'),\n",
      "   (0.013301441, 'weight'),\n",
      "   (0.011307537, 'error'),\n",
      "   (0.010215188, 'gradient'),\n",
      "   (0.009828161, 'network'),\n",
      "   (0.008525674, 'rate'),\n",
      "   (0.0084175905, 'convergence'),\n",
      "   (0.0078820335, 'time'),\n",
      "   (0.0072224797, 'value'),\n",
      "   (0.005991944, 'problem'),\n",
      "   (0.0058140703, 'training'),\n",
      "   (0.0054403744, 'generalization'),\n",
      "   (0.00532522, 'method'),\n",
      "   (0.005148513, 'face'),\n",
      "   (0.005082219, 'stochastic'),\n",
      "   (0.0050216727, 'optimal'),\n",
      "   (0.0049207574, 'figure'),\n",
      "   (0.004787713, 'state'),\n",
      "   (0.004716951, 'term')],\n",
      "  -0.6400774842250718),\n",
      " ([(0.019171119, 'learning'),\n",
      "   (0.016826736, 'network'),\n",
      "   (0.012618786, 'control'),\n",
      "   (0.0119058, 'input'),\n",
      "   (0.010742565, 'output'),\n",
      "   (0.008541439, 'time'),\n",
      "   (0.0081888195, 'dynamic'),\n",
      "   (0.007977988, 'weight'),\n",
      "   (0.0070747053, 'error'),\n",
      "   (0.006402965, 'equation'),\n",
      "   (0.0063500977, 'point'),\n",
      "   (0.0061468557, 'vector'),\n",
      "   (0.005711857, 'unit'),\n",
      "   (0.0056936243, 'figure'),\n",
      "   (0.0056870226, 'training'),\n",
      "   (0.0056078476, 'trajectory'),\n",
      "   (0.0056027817, 'controller'),\n",
      "   (0.0053487257, 'model'),\n",
      "   (0.004973127, 'map'),\n",
      "   (0.004796658, 'value')],\n",
      "  -0.6870654229553573),\n",
      " ([(0.024326935, 'state'),\n",
      "   (0.022106841, 'learning'),\n",
      "   (0.010957599, 'action'),\n",
      "   (0.009881909, 'time'),\n",
      "   (0.009863015, 'value'),\n",
      "   (0.008997031, 'algorithm'),\n",
      "   (0.007886542, 'policy'),\n",
      "   (0.0076155257, 'model'),\n",
      "   (0.007148359, 'problem'),\n",
      "   (0.006460492, 'reinforcement'),\n",
      "   (0.0057255, 'task'),\n",
      "   (0.0054049194, 'step'),\n",
      "   (0.0048458804, 'figure'),\n",
      "   (0.0045060776, 'space'),\n",
      "   (0.004208693, 'optimal'),\n",
      "   (0.0041426383, 'control'),\n",
      "   (0.004070333, 'based'),\n",
      "   (0.0040271925, 'environment'),\n",
      "   (0.0037957607, 'method'),\n",
      "   (0.0037542656, 'reward')],\n",
      "  -0.7763560881921799),\n",
      " ([(0.03559092, 'image'),\n",
      "   (0.013155109, 'feature'),\n",
      "   (0.010730054, 'object'),\n",
      "   (0.010243186, 'data'),\n",
      "   (0.009097157, 'model'),\n",
      "   (0.006404952, 'figure'),\n",
      "   (0.005937373, 'cluster'),\n",
      "   (0.005424224, 'representation'),\n",
      "   (0.0053609097, 'pixel'),\n",
      "   (0.00492016, 'region'),\n",
      "   (0.0048079574, 'structure'),\n",
      "   (0.004559205, 'information'),\n",
      "   (0.0045184167, 'source'),\n",
      "   (0.004474336, 'clustering'),\n",
      "   (0.0041388203, 'point'),\n",
      "   (0.0039827996, 'view'),\n",
      "   (0.0039037403, 'scale'),\n",
      "   (0.0038742872, 'scene'),\n",
      "   (0.0038688872, 'approach'),\n",
      "   (0.0038271614, 'algorithm')],\n",
      "  -0.8643959183736813),\n",
      " ([(0.023014914, 'network'),\n",
      "   (0.012483472, 'neuron'),\n",
      "   (0.011533989, 'circuit'),\n",
      "   (0.010775674, 'input'),\n",
      "   (0.008855971, 'analog'),\n",
      "   (0.008683967, 'chip'),\n",
      "   (0.008321844, 'output'),\n",
      "   (0.0077254074, 'weight'),\n",
      "   (0.007369509, 'figure'),\n",
      "   (0.006990867, 'time'),\n",
      "   (0.006745717, 'memory'),\n",
      "   (0.0054288753, 'state'),\n",
      "   (0.0054282774, 'current'),\n",
      "   (0.005109797, 'voltage'),\n",
      "   (0.0049468353, 'bit'),\n",
      "   (0.0041371086, 'implementation'),\n",
      "   (0.0039964216, 'node'),\n",
      "   (0.003939514, 'vlsi'),\n",
      "   (0.0037253129, 'connection'),\n",
      "   (0.0035755015, 'pattern')],\n",
      "  -0.9257955506771988),\n",
      " ([(0.023060774, 'neuron'),\n",
      "   (0.017503532, 'cell'),\n",
      "   (0.013010494, 'model'),\n",
      "   (0.010465002, 'input'),\n",
      "   (0.009200123, 'synaptic'),\n",
      "   (0.0077082478, 'firing'),\n",
      "   (0.007550914, 'pattern'),\n",
      "   (0.0072465567, 'time'),\n",
      "   (0.0071132556, 'activity'),\n",
      "   (0.006283964, 'spike'),\n",
      "   (0.0062721903, 'network'),\n",
      "   (0.005333966, 'figure'),\n",
      "   (0.0046995003, 'response'),\n",
      "   (0.004514326, 'synapsis'),\n",
      "   (0.0044393297, 'potential'),\n",
      "   (0.0039821486, 'rate'),\n",
      "   (0.00397782, 'connection'),\n",
      "   (0.003888598, 'cortex'),\n",
      "   (0.0037360194, 'cortical'),\n",
      "   (0.0035835737, 'excitatory')],\n",
      "  -0.9360999513098875),\n",
      " ([(0.01979945, 'model'),\n",
      "   (0.0116229765, 'visual'),\n",
      "   (0.0077815424, 'field'),\n",
      "   (0.007295059, 'cell'),\n",
      "   (0.0072935186, 'figure'),\n",
      "   (0.007155803, 'input'),\n",
      "   (0.0068165706, 'response'),\n",
      "   (0.006379796, 'stimulus'),\n",
      "   (0.00632216, 'network'),\n",
      "   (0.0062078615, 'orientation'),\n",
      "   (0.00595238, 'unit'),\n",
      "   (0.005884633, 'direction'),\n",
      "   (0.0056230505, 'eye'),\n",
      "   (0.005014889, 'neuron'),\n",
      "   (0.004791007, 'activity'),\n",
      "   (0.00473112, 'movement'),\n",
      "   (0.0045567136, 'object'),\n",
      "   (0.0044307127, 'position'),\n",
      "   (0.004340211, 'motor'),\n",
      "   (0.004312798, 'receptive')],\n",
      "  -1.1079701727305706),\n",
      " ([(0.016383775, 'motion'),\n",
      "   (0.010740515, 'cell'),\n",
      "   (0.010030417, 'field'),\n",
      "   (0.009239079, 'figure'),\n",
      "   (0.008651045, 'image'),\n",
      "   (0.008208994, 'direction'),\n",
      "   (0.0076601203, 'code'),\n",
      "   (0.006636992, 'pixel'),\n",
      "   (0.0060744365, 'channel'),\n",
      "   (0.005770007, 'edge'),\n",
      "   (0.005547678, 'time'),\n",
      "   (0.00517235, 'retina'),\n",
      "   (0.004927771, 'velocity'),\n",
      "   (0.0047931997, 'contour'),\n",
      "   (0.004339921, 'output'),\n",
      "   (0.004316583, 'stage'),\n",
      "   (0.0041560037, 'intensity'),\n",
      "   (0.0041114474, 'line'),\n",
      "   (0.00356499, 'call'),\n",
      "   (0.0035461658, 'feature')],\n",
      "  -1.190429444382718),\n",
      " ([(0.018400341, 'representation'),\n",
      "   (0.014626018, 'cue'),\n",
      "   (0.013070233, 'sound'),\n",
      "   (0.010569334, 'role'),\n",
      "   (0.010418304, 'component'),\n",
      "   (0.010183906, 'subject'),\n",
      "   (0.01011599, 'binding'),\n",
      "   (0.009342643, 'model'),\n",
      "   (0.008562245, 'structure'),\n",
      "   (0.008478386, 'localization'),\n",
      "   (0.006984182, 'figure'),\n",
      "   (0.0069729784, 'target'),\n",
      "   (0.006246809, 'were'),\n",
      "   (0.0062121553, 'unit'),\n",
      "   (0.0059443642, 'auditory'),\n",
      "   (0.0057778847, 'activity'),\n",
      "   (0.005773929, 'location'),\n",
      "   (0.0054002856, 'connectionist'),\n",
      "   (0.0051551014, 'left'),\n",
      "   (0.0051102103, 'eeg')],\n",
      "  -1.4494740931049632)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 15\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = 50  \n",
    "\n",
    "\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "\n",
    "top_topics = model.top_topics(corpus)\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n",
    "\n",
    "def check_topic_threshold(x, topic,threshold):\n",
    "    topics = model.get_document_topics(corpus[x])\n",
    "    for i in topics:\n",
    "        if i[0]==topic and i[1]>threshold:\n",
    "            return True\n",
    "    else: return False\n",
    "t=[docs[x] for x in range(len(corpus)) if check_topic_threshold(x, 0, .9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bff96a-acca-4d5a-a66b-60018d3227e7",
   "metadata": {},
   "source": [
    "    Interpreting the results after training the LDA model using num_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51382f07-dcb8-438f-9ff0-18186b1c08ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -0.7664.\n",
      "[([(0.04230769, 'network'),\n",
      "   (0.019953296, 'training'),\n",
      "   (0.014985912, 'input'),\n",
      "   (0.014737401, 'unit'),\n",
      "   (0.012810181, 'weight'),\n",
      "   (0.012579819, 'output'),\n",
      "   (0.011903695, 'hidden'),\n",
      "   (0.011820152, 'error'),\n",
      "   (0.0110104345, 'layer'),\n",
      "   (0.008734973, 'net'),\n",
      "   (0.00811618, 'learning'),\n",
      "   (0.007656898, 'time'),\n",
      "   (0.0076067955, 'performance'),\n",
      "   (0.0067791734, 'task'),\n",
      "   (0.0062481873, 'trained'),\n",
      "   (0.005962459, 'were'),\n",
      "   (0.0058288663, 'architecture'),\n",
      "   (0.00565377, 'figure'),\n",
      "   (0.0051911557, 'problem'),\n",
      "   (0.004972692, 'number')],\n",
      "  -0.33725877952929234),\n",
      " ([(0.02583927, 'network'),\n",
      "   (0.02177658, 'rule'),\n",
      "   (0.016142791, 'learning'),\n",
      "   (0.0063816234, 'weight'),\n",
      "   (0.005741633, 'example'),\n",
      "   (0.0049804077, 'training'),\n",
      "   (0.004803268, 'number'),\n",
      "   (0.0041650045, 'approach'),\n",
      "   (0.0041186204, 'our'),\n",
      "   (0.004000623, 'figure'),\n",
      "   (0.0039258953, 'node'),\n",
      "   (0.003924177, 'algorithm'),\n",
      "   (0.0038684248, 'input'),\n",
      "   (0.0037445263, 'problem'),\n",
      "   (0.003573255, 'unit'),\n",
      "   (0.0033887352, 'time'),\n",
      "   (0.00338502, 'knowledge'),\n",
      "   (0.0032808676, 'will'),\n",
      "   (0.0031632974, 'other'),\n",
      "   (0.0031442055, 'level')],\n",
      "  -0.38864705144908596),\n",
      " ([(0.014413348, 'data'),\n",
      "   (0.011599399, 'network'),\n",
      "   (0.010165208, 'input'),\n",
      "   (0.009619849, 'space'),\n",
      "   (0.009392218, 'map'),\n",
      "   (0.008615585, 'feature'),\n",
      "   (0.00825564, 'point'),\n",
      "   (0.0073743328, 'value'),\n",
      "   (0.0067875115, 'method'),\n",
      "   (0.006189929, 'algorithm'),\n",
      "   (0.00580339, 'number'),\n",
      "   (0.0054442026, 'problem'),\n",
      "   (0.0054232464, 'region'),\n",
      "   (0.0053198836, 'dimensional'),\n",
      "   (0.0051558022, 'local'),\n",
      "   (0.0050229835, 'class'),\n",
      "   (0.0049520247, 'learning'),\n",
      "   (0.0048630503, 'figure'),\n",
      "   (0.0047065597, 'structure'),\n",
      "   (0.0046875523, 'example')],\n",
      "  -0.42366924508350434),\n",
      " ([(0.011301275, 'algorithm'),\n",
      "   (0.00872403, 'data'),\n",
      "   (0.008388908, 'distribution'),\n",
      "   (0.007298115, 'learning'),\n",
      "   (0.006846385, 'matrix'),\n",
      "   (0.0066689723, 'gaussian'),\n",
      "   (0.0061494242, 'parameter'),\n",
      "   (0.006002992, 'linear'),\n",
      "   (0.006001838, 'vector'),\n",
      "   (0.005584129, 'model'),\n",
      "   (0.0052562994, 'noise'),\n",
      "   (0.005204879, 'method'),\n",
      "   (0.0051044514, 'case'),\n",
      "   (0.004840491, 'mean'),\n",
      "   (0.004729257, 'approximation'),\n",
      "   (0.0046678935, 'point'),\n",
      "   (0.004638945, 'given'),\n",
      "   (0.004635397, 'value'),\n",
      "   (0.004484712, 'problem'),\n",
      "   (0.0044312906, 'kernel')],\n",
      "  -0.5040474292232376),\n",
      " ([(0.027196832, 'state'),\n",
      "   (0.023681248, 'learning'),\n",
      "   (0.013473246, 'action'),\n",
      "   (0.012509608, 'value'),\n",
      "   (0.01227354, 'time'),\n",
      "   (0.010446851, 'policy'),\n",
      "   (0.009481297, 'algorithm'),\n",
      "   (0.008877392, 'reinforcement'),\n",
      "   (0.0070038885, 'problem'),\n",
      "   (0.006068596, 'step'),\n",
      "   (0.0058995914, 'optimal'),\n",
      "   (0.0054968735, 'task'),\n",
      "   (0.0053226594, 'control'),\n",
      "   (0.0047774212, 'reward'),\n",
      "   (0.004698266, 'figure'),\n",
      "   (0.0041566463, 'based'),\n",
      "   (0.0040843464, 'method'),\n",
      "   (0.004036844, 'environment'),\n",
      "   (0.0039699925, 'model'),\n",
      "   (0.003876312, 'space')],\n",
      "  -0.63901856642745),\n",
      " ([(0.033537176, 'unit'),\n",
      "   (0.027524767, 'network'),\n",
      "   (0.017606402, 'input'),\n",
      "   (0.013891402, 'learning'),\n",
      "   (0.012137159, 'weight'),\n",
      "   (0.011211276, 'output'),\n",
      "   (0.009584886, 'pattern'),\n",
      "   (0.009481917, 'hidden'),\n",
      "   (0.008503067, 'state'),\n",
      "   (0.007301191, 'layer'),\n",
      "   (0.005786414, 'recurrent'),\n",
      "   (0.005709394, 'representation'),\n",
      "   (0.0055419463, 'time'),\n",
      "   (0.0054266714, 'figure'),\n",
      "   (0.00482513, 'node'),\n",
      "   (0.004720752, 'sequence'),\n",
      "   (0.0047080466, 'training'),\n",
      "   (0.004570394, 'net'),\n",
      "   (0.00442604, 'order'),\n",
      "   (0.0043451255, 'activation')],\n",
      "  -0.65011001874594),\n",
      " ([(0.014730353, 'network'),\n",
      "   (0.011458767, 'problem'),\n",
      "   (0.010826983, 'algorithm'),\n",
      "   (0.01063126, 'learning'),\n",
      "   (0.010439297, 'equation'),\n",
      "   (0.008652368, 'point'),\n",
      "   (0.008447306, 'gradient'),\n",
      "   (0.0080190655, 'time'),\n",
      "   (0.0076430268, 'solution'),\n",
      "   (0.0065218434, 'method'),\n",
      "   (0.0062735304, 'weight'),\n",
      "   (0.0060013058, 'energy'),\n",
      "   (0.005927869, 'convergence'),\n",
      "   (0.005510051, 'minimum'),\n",
      "   (0.005472791, 'error'),\n",
      "   (0.005453538, 'matrix'),\n",
      "   (0.005218036, 'term'),\n",
      "   (0.0051042167, 'dynamic'),\n",
      "   (0.005059299, 'optimization'),\n",
      "   (0.0050415625, 'will')],\n",
      "  -0.6688953431707636),\n",
      " ([(0.03300801, 'model'),\n",
      "   (0.015658082, 'data'),\n",
      "   (0.013078536, 'network'),\n",
      "   (0.010930871, 'probability'),\n",
      "   (0.010041619, 'parameter'),\n",
      "   (0.007931341, 'mixture'),\n",
      "   (0.007418199, 'distribution'),\n",
      "   (0.0068625, 'variable'),\n",
      "   (0.0063794856, 'likelihood'),\n",
      "   (0.0057771476, 'method'),\n",
      "   (0.005776112, 'state'),\n",
      "   (0.0049417233, 'given'),\n",
      "   (0.00485016, 'algorithm'),\n",
      "   (0.0047008204, 'density'),\n",
      "   (0.004647174, 'estimate'),\n",
      "   (0.00461508, 'hidden'),\n",
      "   (0.0045372657, 'prediction'),\n",
      "   (0.0044784057, 'training'),\n",
      "   (0.0044585066, 'approach'),\n",
      "   (0.0043013035, 'bayesian')],\n",
      "  -0.6925176340591387),\n",
      " ([(0.024609303, 'network'),\n",
      "   (0.010146249, 'weight'),\n",
      "   (0.009914168, 'input'),\n",
      "   (0.009570402, 'bound'),\n",
      "   (0.009207008, 'threshold'),\n",
      "   (0.00918805, 'number'),\n",
      "   (0.008880443, 'theorem'),\n",
      "   (0.008137426, 'node'),\n",
      "   (0.0076761083, 'any'),\n",
      "   (0.0075938604, 'then'),\n",
      "   (0.0073867086, 'let'),\n",
      "   (0.006812303, 'output'),\n",
      "   (0.006451598, 'proof'),\n",
      "   (0.006445845, 'there'),\n",
      "   (0.005892857, 'polynomial'),\n",
      "   (0.0058892043, 'class'),\n",
      "   (0.005761289, 'net'),\n",
      "   (0.005627651, 'layer'),\n",
      "   (0.0053601023, 'linear'),\n",
      "   (0.005264856, 'will')],\n",
      "  -0.6932947591034232),\n",
      " ([(0.017490147, 'cell'),\n",
      "   (0.016954003, 'model'),\n",
      "   (0.011893529, 'stimulus'),\n",
      "   (0.010638897, 'response'),\n",
      "   (0.008758334, 'visual'),\n",
      "   (0.007153186, 'direction'),\n",
      "   (0.00695497, 'figure'),\n",
      "   (0.006193182, 'orientation'),\n",
      "   (0.0060999496, 'eye'),\n",
      "   (0.005542879, 'field'),\n",
      "   (0.005514664, 'cortex'),\n",
      "   (0.0053255707, 'frequency'),\n",
      "   (0.0053132274, 'neuron'),\n",
      "   (0.0050276574, 'spatial'),\n",
      "   (0.0048950207, 'activity'),\n",
      "   (0.00478853, 'motion'),\n",
      "   (0.0047480282, 'input'),\n",
      "   (0.004674719, 'were'),\n",
      "   (0.0046053105, 'unit'),\n",
      "   (0.004340039, 'time')],\n",
      "  -0.699281652189392),\n",
      " ([(0.01423235, 'error'),\n",
      "   (0.012862696, 'training'),\n",
      "   (0.012234822, 'learning'),\n",
      "   (0.012021321, 'example'),\n",
      "   (0.009199908, 'class'),\n",
      "   (0.008422362, 'algorithm'),\n",
      "   (0.0071070106, 'generalization'),\n",
      "   (0.006974563, 'sample'),\n",
      "   (0.0069727073, 'probability'),\n",
      "   (0.006875478, 'data'),\n",
      "   (0.0068454915, 'tree'),\n",
      "   (0.006388372, 'number'),\n",
      "   (0.005656671, 'size'),\n",
      "   (0.0055542453, 'classifier'),\n",
      "   (0.0048534186, 'distribution'),\n",
      "   (0.004767993, 'problem'),\n",
      "   (0.004741145, 'hypothesis'),\n",
      "   (0.0046820147, 'will'),\n",
      "   (0.0045746285, 'decision'),\n",
      "   (0.004533015, 'test')],\n",
      "  -0.7027319275856074),\n",
      " ([(0.019010913, 'speech'),\n",
      "   (0.01585363, 'classifier'),\n",
      "   (0.0139405625, 'training'),\n",
      "   (0.013633311, 'recognition'),\n",
      "   (0.010785146, 'word'),\n",
      "   (0.009896751, 'classification'),\n",
      "   (0.009874769, 'feature'),\n",
      "   (0.009640008, 'were'),\n",
      "   (0.009113269, 'performance'),\n",
      "   (0.0084854765, 'class'),\n",
      "   (0.007994189, 'network'),\n",
      "   (0.007823947, 'speaker'),\n",
      "   (0.007458256, 'data'),\n",
      "   (0.0057787145, 'error'),\n",
      "   (0.005404317, 'frame'),\n",
      "   (0.0053094635, 'context'),\n",
      "   (0.0052476237, 'time'),\n",
      "   (0.0052352827, 'acoustic'),\n",
      "   (0.0052148714, 'rate'),\n",
      "   (0.005121674, 'test')],\n",
      "  -0.7633282730694221),\n",
      " ([(0.02111658, 'neuron'),\n",
      "   (0.016323831, 'spike'),\n",
      "   (0.016256176, 'memory'),\n",
      "   (0.012356293, 'time'),\n",
      "   (0.011104082, 'model'),\n",
      "   (0.009939782, 'network'),\n",
      "   (0.009863942, 'noise'),\n",
      "   (0.009822525, 'information'),\n",
      "   (0.009072213, 'state'),\n",
      "   (0.0070264637, 'rate'),\n",
      "   (0.0068598785, 'firing'),\n",
      "   (0.006370172, 'capacity'),\n",
      "   (0.006359597, 'input'),\n",
      "   (0.006267864, 'signal'),\n",
      "   (0.006088306, 'pattern'),\n",
      "   (0.00580235, 'probability'),\n",
      "   (0.005094551, 'channel'),\n",
      "   (0.004834323, 'dynamic'),\n",
      "   (0.0047294185, 'value'),\n",
      "   (0.004572712, 'train')],\n",
      "  -0.7722259149011285),\n",
      " ([(0.025121756, 'neuron'),\n",
      "   (0.013638729, 'input'),\n",
      "   (0.012884433, 'network'),\n",
      "   (0.012082867, 'synaptic'),\n",
      "   (0.011765757, 'cell'),\n",
      "   (0.010812767, 'model'),\n",
      "   (0.008509427, 'connection'),\n",
      "   (0.008109044, 'time'),\n",
      "   (0.007294321, 'activity'),\n",
      "   (0.006732574, 'pattern'),\n",
      "   (0.006109931, 'figure'),\n",
      "   (0.0060614883, 'synapsis'),\n",
      "   (0.0049105547, 'excitatory'),\n",
      "   (0.0047945213, 'potential'),\n",
      "   (0.004774656, 'learning'),\n",
      "   (0.004527328, 'inhibitory'),\n",
      "   (0.004255265, 'firing'),\n",
      "   (0.0040973006, 'phase'),\n",
      "   (0.0038427035, 'output'),\n",
      "   (0.0037576698, 'simulation')],\n",
      "  -0.8605694813638541),\n",
      " ([(0.042163655, 'image'),\n",
      "   (0.021543603, 'object'),\n",
      "   (0.013427424, 'feature'),\n",
      "   (0.0100353565, 'visual'),\n",
      "   (0.008713288, 'figure'),\n",
      "   (0.008159789, 'face'),\n",
      "   (0.007769429, 'pixel'),\n",
      "   (0.0067205746, 'motion'),\n",
      "   (0.0065772883, 'view'),\n",
      "   (0.0064536147, 'recognition'),\n",
      "   (0.00613365, 'model'),\n",
      "   (0.0058658766, 'representation'),\n",
      "   (0.005601693, 'vision'),\n",
      "   (0.0055757314, 'field'),\n",
      "   (0.005078483, 'scene'),\n",
      "   (0.004877651, 'edge'),\n",
      "   (0.004603552, 'scale'),\n",
      "   (0.0045927214, 'based'),\n",
      "   (0.0043147784, 'filter'),\n",
      "   (0.004277726, 'information')],\n",
      "  -0.8643511164090889),\n",
      " ([(0.036990855, 'model'),\n",
      "   (0.023410443, 'control'),\n",
      "   (0.01242626, 'trajectory'),\n",
      "   (0.011870696, 'learning'),\n",
      "   (0.01109257, 'motor'),\n",
      "   (0.009589627, 'controller'),\n",
      "   (0.009001787, 'movement'),\n",
      "   (0.008425625, 'robot'),\n",
      "   (0.00795549, 'dynamic'),\n",
      "   (0.007209649, 'arm'),\n",
      "   (0.0070798583, 'figure'),\n",
      "   (0.0063508605, 'forward'),\n",
      "   (0.0061968337, 'network'),\n",
      "   (0.0061914446, 'position'),\n",
      "   (0.005712561, 'point'),\n",
      "   (0.0056641293, 'data'),\n",
      "   (0.0050353515, 'inverse'),\n",
      "   (0.004893868, 'hand'),\n",
      "   (0.004809972, 'command'),\n",
      "   (0.0047439737, 'time')],\n",
      "  -0.9489169794630801),\n",
      " ([(0.01856213, 'circuit'),\n",
      "   (0.01671069, 'chip'),\n",
      "   (0.013534907, 'analog'),\n",
      "   (0.012490729, 'figure'),\n",
      "   (0.01234771, 'output'),\n",
      "   (0.012021145, 'input'),\n",
      "   (0.01079285, 'signal'),\n",
      "   (0.010385893, 'voltage'),\n",
      "   (0.010242135, 'current'),\n",
      "   (0.009076618, 'time'),\n",
      "   (0.0075874426, 'vlsi'),\n",
      "   (0.007294327, 'network'),\n",
      "   (0.0061036935, 'implementation'),\n",
      "   (0.0051938863, 'weight'),\n",
      "   (0.005099414, 'neuron'),\n",
      "   (0.004954735, 'pulse'),\n",
      "   (0.0046221446, 'transistor'),\n",
      "   (0.004590387, 'digital'),\n",
      "   (0.004473305, 'processing'),\n",
      "   (0.0044377833, 'bit')],\n",
      "  -0.9663898723297047),\n",
      " ([(0.018244116, 'vector'),\n",
      "   (0.014093756, 'distance'),\n",
      "   (0.013860413, 'character'),\n",
      "   (0.011920948, 'recognition'),\n",
      "   (0.009454429, 'word'),\n",
      "   (0.009233392, 'image'),\n",
      "   (0.008811998, 'digit'),\n",
      "   (0.0072512054, 'pattern'),\n",
      "   (0.0071158754, 'transformation'),\n",
      "   (0.007023764, 'code'),\n",
      "   (0.0063938354, 'representation'),\n",
      "   (0.006380329, 'tangent'),\n",
      "   (0.006355423, 'data'),\n",
      "   (0.0056683444, 'feature'),\n",
      "   (0.004454589, 'model'),\n",
      "   (0.004265281, 'algorithm'),\n",
      "   (0.0042308723, 'handwritten'),\n",
      "   (0.0042109676, 'method'),\n",
      "   (0.004169684, 'prototype'),\n",
      "   (0.0041565886, 'matrix')],\n",
      "  -1.0276197081647467),\n",
      " ([(0.012103544, 'control'),\n",
      "   (0.01090197, 'adaptation'),\n",
      "   (0.010173621, 'light'),\n",
      "   (0.010065187, 'receptor'),\n",
      "   (0.008599736, 'cell'),\n",
      "   (0.008231773, 'retina'),\n",
      "   (0.007968722, 'algorithm'),\n",
      "   (0.007933094, 'feedback'),\n",
      "   (0.0075408085, 'response'),\n",
      "   (0.0069966014, 'gain'),\n",
      "   (0.0064197797, 'change'),\n",
      "   (0.006292252, 'cone'),\n",
      "   (0.006256617, 'ga'),\n",
      "   (0.0057909237, 'intensity'),\n",
      "   (0.0057329168, 'gradient'),\n",
      "   (0.005512429, 'time'),\n",
      "   (0.0048555443, 'loop'),\n",
      "   (0.0046958444, 'figure'),\n",
      "   (0.004300644, 'effect'),\n",
      "   (0.0041526337, 'adaptive')],\n",
      "  -1.3519946302621864),\n",
      " ([(0.015764736, 'vector'),\n",
      "   (0.010420761, 'student'),\n",
      "   (0.010083936, 'field'),\n",
      "   (0.010060275, 'teacher'),\n",
      "   (0.008477009, 'learning'),\n",
      "   (0.00803027, 'input'),\n",
      "   (0.0075950054, 'eeg'),\n",
      "   (0.0067669298, 'unit'),\n",
      "   (0.006625666, 'map'),\n",
      "   (0.0063756187, 'subject'),\n",
      "   (0.0062037716, 'generalization'),\n",
      "   (0.0058507924, 'data'),\n",
      "   (0.0057956236, 'error'),\n",
      "   (0.0056982925, 'ica'),\n",
      "   (0.005550046, 'component'),\n",
      "   (0.005355253, 'self'),\n",
      "   (0.0052732727, 'fig'),\n",
      "   (0.005092948, 'phase'),\n",
      "   (0.0049372097, 'output'),\n",
      "   (0.004843151, 'time')],\n",
      "  -1.3729053973809353)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 20\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = 50  \n",
    "\n",
    "\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "\n",
    "top_topics = model.top_topics(corpus)\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n",
    "\n",
    "def check_topic_threshold(x, topic,threshold):\n",
    "    topics = model.get_document_topics(corpus[x])\n",
    "    for i in topics:\n",
    "        if i[0]==topic and i[1]>threshold:\n",
    "            return True\n",
    "    else: return False\n",
    "t=[docs[x] for x in range(len(corpus)) if check_topic_threshold(x, 0, .9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57ca41-7153-42c8-a58c-d429094e19bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
