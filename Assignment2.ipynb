{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3af535-1373-44d8-b870-99fe88da89eb",
   "metadata": {},
   "source": [
    "## Assignment-2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b84ec-a56a-4e2e-9bcb-6afbdf140d88",
   "metadata": {},
   "source": [
    "Chapter 3.\n",
    "\n",
    "Q6. Describe the class of strings matched by the following regular expressions: \n",
    "a. [a-zA-Z]+\n",
    "\n",
    "b. [A-Z][a-z]* \n",
    "\n",
    "c. p[aeiou]{,2}t\n",
    "\n",
    "d. \\d+(\\.\\d+)?\n",
    "\n",
    "e. ([^aeiou][aeiou][^aeiou])*\n",
    "\n",
    "f. \\w+|[^\\w\\s]+\n",
    "\n",
    "Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017844ae-1981-4b26-937a-4cb160404073",
   "metadata": {},
   "source": [
    "All the given regular expressions could be described as below:\n",
    "\n",
    "[a-zA-Z]+: This regular expression matches one or more consecutive alphabetic characters (both uppercase and lowercase).\n",
    "\n",
    "[A-Z][a-z]*: This regular expression matches an uppercase letter followed by zero or more lowercase letters, effectively representing capitalized words.\n",
    "\n",
    "p[aeiou]{,2}t: This regular expression matches strings that start with 'p', followed by zero to two vowels, and ending with 't'. It matches words like \"pot\", \"peat\", \"put\", etc.\n",
    "\n",
    "\\d+(.\\d+)?: This regular expression matches numbers with optional decimal points. It starts with one or more digits (\\d+), followed by an optional group (.\\d+), representing the decimal part.\n",
    "\n",
    "([^aeiou][aeiou][^aeiou])*: This regular expression matches sequences of three characters where the first and last characters are consonants (not a vowel), and the middle character is a vowel. It can match words like \"cat\", \"dog\", \"rat\", etc.\n",
    "\n",
    " \\w+|[^\\w\\s]+: This regular expression matches words (\\w+) or sequences of non-word, non-space characters ([^\\w\\s]+).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c00b141-5286-492e-8828-521c1dabc2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Expression: [a-zA-Z]+\n",
      "{Hello} {World}\n",
      "\n",
      "{apple} {Banana}\n",
      "\n",
      "{pat} {pit} {pot} {put}\n",
      "\n",
      "123 45.67\n",
      "\n",
      "{cat} {dog} {rat}\n",
      "\n",
      "{alpha}${bravo}_{charlie}\n",
      "\n",
      "Regular Expression: [A-Z][a-z]*\n",
      "{Hello} {World}\n",
      "\n",
      "apple {Banana}\n",
      "\n",
      "pat pit pot put\n",
      "\n",
      "123 45.67\n",
      "\n",
      "cat dog rat\n",
      "\n",
      "alpha$bravo_charlie\n",
      "\n",
      "Regular Expression: p[aeiou]{,2}t\n",
      "Hello World\n",
      "\n",
      "apple Banana\n",
      "\n",
      "{pat} {pit} {pot} {put}\n",
      "\n",
      "123 45.67\n",
      "\n",
      "cat dog rat\n",
      "\n",
      "alpha$bravo_charlie\n",
      "\n",
      "Regular Expression: \\d+(\\.\\d+)?\n",
      "Hello World\n",
      "\n",
      "apple Banana\n",
      "\n",
      "pat pit pot put\n",
      "\n",
      "{123} {45.67}\n",
      "\n",
      "cat dog rat\n",
      "\n",
      "alpha$bravo_charlie\n",
      "\n",
      "Regular Expression: ([^aeiou][aeiou][^aeiou])*\n",
      "{Hello Wor}{}l{}d{}\n",
      "\n",
      "{}a{}p{}p{le Ban}{}a{}n{}a{}\n",
      "\n",
      "{pat}{} {pit}{} {pot}{} {put}{}\n",
      "\n",
      "{}1{}2{}3{} {}4{}5{}.{}6{}7{}\n",
      "\n",
      "{cat}{} {dog}{} {rat}{}\n",
      "\n",
      "{}a{}l{}p{ha$}{}b{rav}{}o{}_{}c{har}{}l{}i{}e{}\n",
      "\n",
      "Regular Expression: \\w+|[^\\w\\s]+\n",
      "{Hello} {World}\n",
      "\n",
      "{apple} {Banana}\n",
      "\n",
      "{pat} {pit} {pot} {put}\n",
      "\n",
      "{123} {45}{.}{67}\n",
      "\n",
      "{cat} {dog} {rat}\n",
      "\n",
      "{alpha}{$}{bravo_charlie}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#Sample strings for testing.\n",
    "strings = [\n",
    "    \"Hello World\",\n",
    "    \"apple Banana\",\n",
    "    \"pat pit pot put\",\n",
    "    \"123 45.67\",\n",
    "    \"cat dog rat\",\n",
    "    \"alpha$bravo_charlie\",\n",
    "]\n",
    "\n",
    "#Testing the given regular expressions.\n",
    "for regex in [r'[a-zA-Z]+', r'[A-Z][a-z]*', r'p[aeiou]{,2}t', r'\\d+(\\.\\d+)?', r'([^aeiou][aeiou][^aeiou])*', r'\\w+|[^\\w\\s]+']:\n",
    "    print(f\"Regular Expression: {regex}\")\n",
    "    for string in strings:\n",
    "        nltk.re_show(regex, string)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663c5f1-5cc4-4d42-909b-679f0f07a7c1",
   "metadata": {},
   "source": [
    "Q7. Write regular expressions to match the following classes of strings:\n",
    "\n",
    "a. A single determiner (assume that a, an, and the are the only determiners)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48196f8e-7cb8-48b8-9197-02194cc06dc6",
   "metadata": {},
   "source": [
    "b. An arithmetic expression using integers, addition, and multiplication, such as\n",
    "2*3+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d3ff98-1129-41fe-aa1d-cafbda5a1280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'a' matches a single determiner.\n",
      "'an' matches a single determiner.\n",
      "'the' matches a single determiner.\n",
      "'2*3+8' matches an arithmetic expression.\n",
      "'3+5*7' matches an arithmetic expression.\n",
      "'an apple' does not match any pattern.\n",
      "'the cat' does not match any pattern.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Sample strings\n",
    "sample_strings = [\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"2*3+8\",\n",
    "    \"3+5*7\",\n",
    "    \"an apple\",\n",
    "    \"the cat\",\n",
    "]\n",
    "\n",
    "#Regular expression for a single determiner\n",
    "determiner_regex = r'\\b(?:a|an|the)\\b'\n",
    "\n",
    "#Regular expression for an arithmetic expression\n",
    "arithmetic_regex = r'\\d+\\s*([*+]\\s*\\d+\\s*)+'\n",
    "\n",
    "#Testing the regular expressions\n",
    "for string in sample_strings:\n",
    "    #Matching determiner\n",
    "    if re.fullmatch(determiner_regex, string):\n",
    "        print(f\"'{string}' matches a single determiner.\")\n",
    "    #Matching arithmetic expression\n",
    "    elif re.fullmatch(arithmetic_regex, string):\n",
    "        print(f\"'{string}' matches an arithmetic expression.\")\n",
    "    # No matchings found.\n",
    "    else:\n",
    "        print(f\"'{string}' does not match any pattern.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddce3b-6b42-4ebb-8174-8628942c4bc7",
   "metadata": {},
   "source": [
    "\n",
    "Q21. Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that web page. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd1dde62-4c05-46dd-9360-135547426c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/db/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doctype', 'html', 'html', 'charset', 'utf', 'http', 'equiv', 'html', 'charset', 'utf', 'viewport', 'css', 'ui', 'blinkmacsystemfont', 'segoe', 'ui', 'helvetica', 'neue', 'helvetica', 'arial', 'fdfdff', 'rgba', 'visited', 'max', 'examples', 'documents', 'coordination', 'asking', 'href', 'https', 'www', 'iana', 'org', 'domains', 'html']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from nltk.corpus import words\n",
    "\n",
    "def unknown(url):\n",
    "    #Fetching HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    #Extracting all substrings consisting of lowercase letters\n",
    "    lowercase_words = re.findall(r'\\b[a-z]+\\b', html_content.lower())\n",
    "    \n",
    "    #Filtering out known words using NLTK's words corpus\n",
    "    known_words = set(words.words())\n",
    "    unknown_words = [word for word in lowercase_words if word not in known_words]\n",
    "    \n",
    "    return unknown_words\n",
    "\n",
    "#Example usage:\n",
    "url = \"https://example.com\"\n",
    "unknown_words = unknown(url)\n",
    "print(unknown_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49176bd0-2cb7-4700-9339-63caf5623d83",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q30. Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer, and see if you ob- serve any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09313188-2820-43f2-937d-5429d0da5cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['running', 'jumps', 'easily', 'happily', 'quickly']\n",
      "Porter stems: ['run', 'jump', 'easili', 'happili', 'quickli']\n",
      "Lancaster stems: ['run', 'jump', 'easy', 'happy', 'quick']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "#Tokenized text\n",
    "tokens = [\"running\", \"jumps\", \"easily\", \"happily\", \"quickly\"]\n",
    "\n",
    "#Initializing stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "#Stemming using Porter Stemmer\n",
    "porter_stems = [porter_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "#Stemming using Lancaster Stemmer\n",
    "lancaster_stems = [lancaster_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "#Printing the results\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Porter stems:\", porter_stems)\n",
    "print(\"Lancaster stems:\", lancaster_stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5807a-498a-4be5-9660-f39470fde0a0",
   "metadata": {},
   "source": [
    "Q39. Read the Wikipedia entry on Soundex. Implement this algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bc34ac7-ce7b-4600-bfaa-8515ac7cb054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2523\n"
     ]
    }
   ],
   "source": [
    "class Soundex:\n",
    "    def __init__(self):\n",
    "        self.soundex_mapping = {\n",
    "            'b': 1, 'f': 1, 'p': 1, 'v': 1,\n",
    "            'c': 2, 'g': 2, 'j': 2, 'k': 2, 'q': 2, 's': 2, 'x': 2, 'z': 2,\n",
    "            'd': 3, 't': 3,\n",
    "            'l': 4,\n",
    "            'm': 5, 'n': 5,\n",
    "            'r': 6\n",
    "        }\n",
    "\n",
    "    def soundex(self, word):\n",
    "        #Converting word to lowercase\n",
    "        word = word.lower()\n",
    "\n",
    "        #Removing non-alphabetic characters\n",
    "        word = ''.join(filter(str.isalpha, word))\n",
    "\n",
    "        #Keeping the first letter\n",
    "        soundex_code = word[0]\n",
    "\n",
    "        #Replaceing consonants with digits\n",
    "        for char in word[1:]:\n",
    "            if char in self.soundex_mapping:\n",
    "                digit = self.soundex_mapping[char]\n",
    "                if digit != soundex_code[-1]:\n",
    "                    soundex_code += str(digit)\n",
    "\n",
    "        #Removing vowels and 'h', 'w', 'y' after the first letter\n",
    "        soundex_code = soundex_code.replace('a', '').replace('e', '').replace('i', '').replace('o', '').replace('u', '')\n",
    "        soundex_code = soundex_code.replace('h', '').replace('w', '').replace('y', '')\n",
    "\n",
    "        #padding with zeros to 3 digits\n",
    "        soundex_code += '000'\n",
    "        soundex_code = soundex_code[:4]\n",
    "\n",
    "        return soundex_code\n",
    "\n",
    "\n",
    "#Example usage:\n",
    "s = Soundex()\n",
    "print(s.soundex(\"Washington\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26fcb99-74a9-4a2d-8d88-1b973b7f2001",
   "metadata": {},
   "source": [
    "\n",
    "        chapter 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ee7d9-766f-4fd0-b340-6b77b7e11554",
   "metadata": {},
   "source": [
    "Q14. Write a function novel10(text) that prints any word that appeared in the last 10% of a text that had not been encountered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4531314c-22e3-46f1-9158-013b0ecab356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "encountered\n",
      "earlier.\n"
     ]
    }
   ],
   "source": [
    "def novel10(text):\n",
    "    #Tokenizing the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    #Calculating the index where the last 10% of the words start\n",
    "    last_10_percent_index = int(len(words) * 0.9)\n",
    "    \n",
    "    #Extracting the words from the last 10% of the text\n",
    "    last_10_percent_words = words[last_10_percent_index:]\n",
    "    \n",
    "    #Initializing a set to store unique words encountered in the first 90% of the text\n",
    "    unique_words = set()\n",
    "    \n",
    "    #Iterating through the words in the first 90% of the text and add them to the set\n",
    "    for word in words[:last_10_percent_index]:\n",
    "        unique_words.add(word)\n",
    "    \n",
    "    #Printing any word from the last 10% of the text that hasn't been encountered earlier\n",
    "    for word in last_10_percent_words:\n",
    "        if word not in unique_words:\n",
    "            print(word)\n",
    "\n",
    "#Example usage:\n",
    "text = \"This is a sample text for testing the novel10 function. It prints any word that appeared in the last 10% of a text that had not been encountered earlier.\"\n",
    "novel10(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea24917-43b3-43ae-9c00-3aa2e8a6f25e",
   "metadata": {},
   "source": [
    "\n",
    "Q20. Write a function that takes a list of words (containing duplicates) and returns a list of words (with no duplicates) sorted by decreasing frequency. E.g., if the input list contained 10 instances of the word table and 9 instances of the word chair, then table would appear before chair in the output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ed375fd-f248-4a88-97c7-c8fd103e33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table', 'chair', 'lamp', 'desk']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sort_words_by_frequency(words):\n",
    "    #Counting the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    #Sortting the words by frequency in decreasing order\n",
    "    sorted_words = sorted(word_counts, key=lambda x: (-word_counts[x], x))\n",
    "    \n",
    "    return sorted_words\n",
    "\n",
    "# Example usage:\n",
    "input_words = [\"table\", \"chair\", \"table\", \"table\", \"chair\", \"desk\", \"chair\", \"table\", \"lamp\", \"chair\", \"table\", \"lamp\"]\n",
    "output_words = sort_words_by_frequency(input_words)\n",
    "print(output_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe456c7-8b07-455a-a70d-31bade3d3b13",
   "metadata": {},
   "source": [
    "Q25. Read about string edit distance and the Levenshtein Algorithm. Try the imple- mentation provided in nltk.edit_dist(). In what way is this using dynamic pro- gramming? Does it use the bottom-up or top-down approach? (See also http:// norvig.com/spell-correct.html.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01658203-6187-407f-9497-677dc1fb4b7e",
   "metadata": {},
   "source": [
    "The Levenshtein distance, also known as the edit distance, measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into another. The NLTK implementation of the Levenshtein distance in the nltk.edit_distance() function uses dynamic programming to efficiently compute the distance between two strings.\n",
    "\n",
    "Dynamic programming is used to solve problems by breaking them down into smaller subproblems and solving each subproblem only once, storing the results to avoid redundant computation. The Levenshtein distance problem exhibits optimal substructure, meaning that the optimal solution to the problem can be constructed from optimal solutions to its subproblems.\n",
    "\n",
    "The NLTK implementation of the Levenshtein distance uses a bottom-up dynamic programming approach. It iterates through the characters of both strings, filling in a matrix where each cell represents the minimum edit distance between substrings of the input strings. By computing the distances from smaller substrings to larger ones, the algorithm builds up to the final solution efficiently. This approach ensures that each subproblem is solved only once and avoids redundant computations, making it an effective solution for computing the Levenshtein distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e82f077-531f-4fa8-baaa-1d1b2ac3834a",
   "metadata": {},
   "source": [
    "Q26. The Catalan numbers arise in many applications of combinatorial mathematics, including the counting of parse trees (Section 8.6). The series can be defined as follows: C0 = 1, and Cn+1 = Σ0..n (CiCn-i).\n",
    "\n",
    "a. Write a recursive function to compute nth Catalan number Cn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a37ae0-e755-4c5d-86b0-8c37577b0479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalan number for n = 5 is 42\n"
     ]
    }
   ],
   "source": [
    "#Defining the recursive function to compute nth Catalan number\n",
    "def catalan_recursive(n):\n",
    "    #Base case: C0 = 1\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        catalan = 0\n",
    "        #Calculating Cn using the recursive formula Cn+1 = Σ0..n (CiCn-i)\n",
    "        for i in range(n):\n",
    "            catalan += catalan_recursive(i) * catalan_recursive(n - i - 1)\n",
    "        return catalan\n",
    "\n",
    "#Defineing the value of n for which you want to compute the Catalan number\n",
    "n = 5\n",
    "\n",
    "#Computing and print the nth Catalan number\n",
    "print(\"Catalan number for n =\", n, \"is\", catalan_recursive(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c86cc-ce59-4fc7-9fe0-1fa2c50facd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
